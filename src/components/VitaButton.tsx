import { useState, useRef, useEffect } from 'react';
import { Mic, Loader2 } from 'lucide-react';
import { useToast } from '@/hooks/use-toast';
import { supabase } from '@/integrations/supabase/client';
import { isNativePlatform } from '@/utils/platform';

type VitaState = 'idle' | 'listening' | 'processing' | 'speaking';

// Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ SpeechRecognition Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ¾Ð³Ð´Ð° Ð½ÑƒÐ¶Ð½Ð¾
let SpeechRecognitionModule: any = null;

async function loadSpeechRecognition() {
  if (!isNativePlatform() || SpeechRecognitionModule) return SpeechRecognitionModule;
  
  try {
    const module = await import('@capacitor-community/speech-recognition');
    SpeechRecognitionModule = module.SpeechRecognition;
    return SpeechRecognitionModule;
  } catch (error) {
    console.error('Failed to load speech recognition:', error);
    return null;
  }
}

export const VitaButton = () => {
  const [state, setState] = useState<VitaState>('idle');
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recognitionActiveRef = useRef(false);
  const { toast } = useToast();

  // Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ð½Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñ‹
  useEffect(() => {
    if (!isNativePlatform()) return;

    const initNativeSpeechRecognition = async () => {
      try {
        const SpeechRecognition = await loadSpeechRecognition();
        if (!SpeechRecognition) return;

        // Ð—Ð°Ð¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÐ¼ Ñ€Ð°Ð·Ñ€ÐµÑˆÐµÐ½Ð¸Ñ
        const { available } = await SpeechRecognition.available();
        if (!available) {
          console.warn('Speech recognition not available');
          return;
        }

        const permission = await SpeechRecognition.requestPermissions();
        if (permission.speechRecognition !== 'granted') {
          console.warn('Speech recognition permission denied');
          return;
        }

        // Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ðµ Ð¿Ñ€Ð¾ÑÐ»ÑƒÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ wake word
        startWakeWordDetection();
      } catch (error) {
        console.error('Failed to initialize speech recognition:', error);
      }
    };

    initNativeSpeechRecognition();

    return () => {
      stopWakeWordDetection();
    };
  }, []);

  const startWakeWordDetection = async () => {
    if (!isNativePlatform() || recognitionActiveRef.current) return;

    try {
      const SpeechRecognition = await loadSpeechRecognition();
      if (!SpeechRecognition) return;

      recognitionActiveRef.current = true;

      // Ð¡Ð»ÑƒÑˆÐ°Ñ‚ÐµÐ»ÑŒ Ð´Ð»Ñ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
      SpeechRecognition.addListener('partialResults', (data: any) => {
        const text = data.matches?.join(' ').toLowerCase() || '';
        console.log('Wake word detection:', text);

        if (text.includes('Ð²Ð¸Ñ‚Ð°') && state === 'idle') {
          console.log('Wake word detected!');
          stopWakeWordDetection();
          startListening();
        }
      });

      // Ð—Ð°Ð¿ÑƒÑÐº Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ
      await SpeechRecognition.start({
        language: 'ru-RU',
        partialResults: true,
        popup: false,
      });

      console.log('Wake word detection started (native)');
    } catch (error) {
      console.error('Failed to start wake word detection:', error);
      recognitionActiveRef.current = false;
      
      // ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð½Ð°Ñ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° Ñ‡ÐµÑ€ÐµÐ· 2 ÑÐµÐºÑƒÐ½Ð´Ñ‹
      setTimeout(() => {
        if (state === 'idle') startWakeWordDetection();
      }, 2000);
    }
  };

  const stopWakeWordDetection = async () => {
    if (!isNativePlatform() || !recognitionActiveRef.current) return;

    try {
      const SpeechRecognition = await loadSpeechRecognition();
      if (!SpeechRecognition) return;

      await SpeechRecognition.stop();
      SpeechRecognition.removeAllListeners();
      recognitionActiveRef.current = false;
      console.log('Wake word detection stopped');
    } catch (error) {
      console.error('Failed to stop wake word detection:', error);
    }
  };

  const startListening = async () => {
    try {
      setState('listening');
      
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        stream.getTracks().forEach(track => track.stop());
        await processAudio(audioBlob);
      };

      mediaRecorder.start();

      // ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ñ‡ÐµÑ€ÐµÐ· 5 ÑÐµÐºÑƒÐ½Ð´
      setTimeout(() => {
        if (mediaRecorder.state === 'recording') {
          mediaRecorder.stop();
        }
      }, 5000);

    } catch (error) {
      console.error('Error starting recording:', error);
      toast({
        title: "ÐžÑˆÐ¸Ð±ÐºÐ°",
        description: "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ñƒ",
        variant: "destructive"
      });
      setState('idle');
      
      // ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ñ€Ð¾ÑÐ»ÑƒÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ wake word
      if (isNativePlatform()) {
        setTimeout(() => startWakeWordDetection(), 1000);
      }
    }
  };

  const processAudio = async (audioBlob: Blob) => {
    try {
      setState('processing');

      // ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² base64
      const reader = new FileReader();
      reader.readAsDataURL(audioBlob);
      
      reader.onloadend = async () => {
        const base64Audio = reader.result?.toString().split(',')[1];
        
        if (!base64Audio) {
          throw new Error('Failed to convert audio');
        }

        try {
          // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ
          const { data: transcription, error: transcriptionError } = await supabase.functions.invoke('voice-to-text', {
            body: { audio: base64Audio }
          });

          if (transcriptionError) {
            console.error('Transcription error:', transcriptionError);
            throw new Error('Failed to transcribe audio');
          }

          if (!transcription?.text) {
            throw new Error('No transcription text received');
          }

          console.log('Transcribed text:', transcription.text);

          // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð² AI Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð°
          const { data: aiResponse, error: aiError } = await supabase.functions.invoke('ai-assistant', {
            body: { 
              messages: [
                { role: 'user', content: transcription.text }
              ]
            }
          });

          if (aiError) {
            console.error('AI error:', aiError);
            throw new Error('Failed to get AI response');
          }

          if (!aiResponse) {
            throw new Error('No AI response received');
          }

          console.log('AI response:', aiResponse);

          // ÐžÐ·Ð²ÑƒÑ‡Ð¸Ð²Ð°ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚
          setState('speaking');
          await speakResponse(aiResponse.response || aiResponse.message);

        } catch (error: any) {
          console.error('Processing error:', error);
          
          // ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð´Ñ€ÑƒÐ¶ÐµÑÑ‚Ð²ÐµÐ½Ð½ÑƒÑŽ Ð¾ÑˆÐ¸Ð±ÐºÑƒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŽ
          if (error.message?.includes('429') || error.status === 429) {
            toast({
              title: "Ð¡Ð»Ð¸ÑˆÐºÐ¾Ð¼ Ð¼Ð½Ð¾Ð³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²",
              description: "ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð·Ð¶Ðµ",
              variant: "destructive"
            });
          } else if (error.message?.includes('402') || error.status === 402) {
            toast({
              title: "ÐŸÑ€ÐµÐ²Ñ‹ÑˆÐµÐ½ Ð»Ð¸Ð¼Ð¸Ñ‚",
              description: "ÐŸÐ¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ Ð±Ð°Ð»Ð°Ð½Ñ",
              variant: "destructive"
            });
          } else {
            toast({
              title: "ÐžÑˆÐ¸Ð±ÐºÐ°",
              description: "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ",
              variant: "destructive"
            });
          }
          
          setState('idle');
          
          // ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ñ€Ð¾ÑÐ»ÑƒÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ
          if (isNativePlatform()) {
            setTimeout(() => startWakeWordDetection(), 1000);
          }
        }
      };

    } catch (error) {
      console.error('Error processing audio:', error);
      toast({
        title: "ÐžÑˆÐ¸Ð±ÐºÐ°",
        description: "ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ",
        variant: "destructive"
      });
      setState('idle');
      
      // ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ñ€Ð¾ÑÐ»ÑƒÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ
      if (isNativePlatform()) {
        setTimeout(() => startWakeWordDetection(), 1000);
      }
    }
  };

  const speakResponse = async (text: string) => {
    try {
      const { data: audioData, error: audioError } = await supabase.functions.invoke('text-to-speech', {
        body: { text, voice: 'alena' }
      });

      if (audioError || !audioData?.audioContent) {
        throw new Error('Failed to generate speech');
      }

      // Ð’Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ð¼ Ð°ÑƒÐ´Ð¸Ð¾
      const audio = new Audio(`data:audio/mp3;base64,${audioData.audioContent}`);
      
      audio.onended = () => {
        setState('idle');
        // ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ñ€Ð¾ÑÐ»ÑƒÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ wake word Ð¿Ð¾ÑÐ»Ðµ Ð¾Ð·Ð²ÑƒÑ‡ÐºÐ¸
        if (isNativePlatform()) {
          setTimeout(() => startWakeWordDetection(), 500);
        }
      };

      audio.onerror = () => {
        setState('idle');
        if (isNativePlatform()) {
          setTimeout(() => startWakeWordDetection(), 500);
        }
      };

      await audio.play();

    } catch (error) {
      console.error('Error speaking response:', error);
      setState('idle');
      if (isNativePlatform()) {
        setTimeout(() => startWakeWordDetection(), 500);
      }
    }
  };

  const getButtonContent = () => {
    switch (state) {
      case 'listening':
        return (
          <div className="relative">
            <Mic className="w-5 h-5" />
            <span className="absolute inset-0 animate-ping rounded-full bg-current opacity-25" />
          </div>
        );
      case 'processing':
        return <Loader2 className="w-5 h-5 animate-spin" />;
      case 'speaking':
        return (
          <div className="relative">
            <span className="text-lg">ðŸ”Š</span>
            <span className="absolute inset-0 animate-pulse rounded-full bg-current opacity-25" />
          </div>
        );
      default:
        return <Mic className="w-5 h-5" />;
    }
  };

  const getButtonColor = () => {
    switch (state) {
      case 'listening':
        return 'bg-gradient-to-r from-blue-500 to-cyan-500';
      case 'processing':
        return 'bg-gradient-to-r from-orange-500 to-amber-500';
      case 'speaking':
        return 'bg-gradient-to-r from-purple-500 to-pink-500';
      default:
        return 'bg-gradient-to-r from-purple-500 to-pink-500';
    }
  };

  return (
    <button
      onClick={startListening}
      disabled={state !== 'idle'}
      className={`flex items-center gap-1.5 px-3 py-1.5 rounded-xl text-white shadow-md transition-all ${getButtonColor()} ${
        state !== 'idle' ? 'animate-pulse' : 'hover:scale-105'
      }`}
      title={state === 'idle' ? (isNativePlatform() ? 'ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ Ð¸Ð»Ð¸ ÑÐºÐ°Ð¶Ð¸Ñ‚Ðµ "Ð’Ð¸Ñ‚Ð°"' : 'ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸ÑÐ¸') : 'ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°...'}
    >
      {getButtonContent()}
      <span className="text-xs font-semibold">Ð’Ð¸Ñ‚Ð°</span>
    </button>
  );
};
